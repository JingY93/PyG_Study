{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import torch_geometric.nn as gnn\n",
    "import torch_geometric.data as gdata\n",
    "import torch_geometric.datasets as gdatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "cora()\nData(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])\n"
    }
   ],
   "source": [
    "cora_path = \"pyg_datasets/cora\"\n",
    "cora = gdatasets.Planetoid(cora_path, \"cora\")\n",
    "print (cora)\n",
    "print (cora[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ENZYMES(600)\nData(edge_index=[2, 168], x=[37, 3], y=[1])\nData(edge_index=[2, 102], x=[23, 3], y=[1])\n"
    }
   ],
   "source": [
    "enzymes_path = \"pyg_datasets/enzymes\"\n",
    "enzymes = gdatasets.TUDataset(enzymes_path, \"ENZYMES\")\n",
    "print (enzymes)\n",
    "print (enzymes[0])\n",
    "print (enzymes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enzymes_train = enzymes[:540]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0\t, loss: 1.9417, test_acc: 0.4460\nepoch: 1\t, loss: 1.8264, test_acc: 0.4910\nepoch: 2\t, loss: 1.6927, test_acc: 0.5060\nepoch: 3\t, loss: 1.5415, test_acc: 0.4960\nepoch: 4\t, loss: 1.4023, test_acc: 0.5200\nepoch: 5\t, loss: 1.2664, test_acc: 0.5720\nepoch: 6\t, loss: 1.1343, test_acc: 0.6290\nepoch: 7\t, loss: 1.0119, test_acc: 0.6630\nepoch: 8\t, loss: 0.8993, test_acc: 0.6850\nepoch: 9\t, loss: 0.7953, test_acc: 0.7020\nepoch: 10\t, loss: 0.6980, test_acc: 0.7170\nepoch: 11\t, loss: 0.6072, test_acc: 0.7320\nepoch: 12\t, loss: 0.5234, test_acc: 0.7560\nepoch: 13\t, loss: 0.4471, test_acc: 0.7670\nepoch: 14\t, loss: 0.3788, test_acc: 0.7720\nepoch: 15\t, loss: 0.3184, test_acc: 0.7860\nepoch: 16\t, loss: 0.2660, test_acc: 0.7910\nepoch: 17\t, loss: 0.2213, test_acc: 0.7920\nepoch: 18\t, loss: 0.1838, test_acc: 0.7960\nepoch: 19\t, loss: 0.1527, test_acc: 0.7980\nepoch: 20\t, loss: 0.1270, test_acc: 0.7970\nepoch: 21\t, loss: 0.1058, test_acc: 0.7950\nepoch: 22\t, loss: 0.0883, test_acc: 0.7950\nepoch: 23\t, loss: 0.0739, test_acc: 0.7940\nepoch: 24\t, loss: 0.0621, test_acc: 0.7940\nepoch: 25\t, loss: 0.0524, test_acc: 0.7950\nepoch: 26\t, loss: 0.0445, test_acc: 0.7940\nepoch: 27\t, loss: 0.0380, test_acc: 0.7950\nepoch: 28\t, loss: 0.0326, test_acc: 0.7960\nepoch: 29\t, loss: 0.0282, test_acc: 0.7930\nepoch: 30\t, loss: 0.0245, test_acc: 0.7900\nepoch: 31\t, loss: 0.0214, test_acc: 0.7900\nepoch: 32\t, loss: 0.0188, test_acc: 0.7880\nepoch: 33\t, loss: 0.0166, test_acc: 0.7860\nepoch: 34\t, loss: 0.0148, test_acc: 0.7870\nepoch: 35\t, loss: 0.0133, test_acc: 0.7910\nepoch: 36\t, loss: 0.0120, test_acc: 0.7910\nepoch: 37\t, loss: 0.0109, test_acc: 0.7900\nepoch: 38\t, loss: 0.0099, test_acc: 0.7880\nepoch: 39\t, loss: 0.0090, test_acc: 0.7870\nepoch: 40\t, loss: 0.0083, test_acc: 0.7860\nepoch: 41\t, loss: 0.0077, test_acc: 0.7820\nepoch: 42\t, loss: 0.0071, test_acc: 0.7830\nepoch: 43\t, loss: 0.0066, test_acc: 0.7840\nepoch: 44\t, loss: 0.0062, test_acc: 0.7850\nepoch: 45\t, loss: 0.0058, test_acc: 0.7850\nepoch: 46\t, loss: 0.0055, test_acc: 0.7840\nepoch: 47\t, loss: 0.0052, test_acc: 0.7840\nepoch: 48\t, loss: 0.0049, test_acc: 0.7840\nepoch: 49\t, loss: 0.0046, test_acc: 0.7840\nepoch: 50\t, loss: 0.0044, test_acc: 0.7840\nepoch: 51\t, loss: 0.0042, test_acc: 0.7830\nepoch: 52\t, loss: 0.0040, test_acc: 0.7830\nepoch: 53\t, loss: 0.0039, test_acc: 0.7830\nepoch: 54\t, loss: 0.0037, test_acc: 0.7830\nepoch: 55\t, loss: 0.0036, test_acc: 0.7830\nepoch: 56\t, loss: 0.0035, test_acc: 0.7820\nepoch: 57\t, loss: 0.0034, test_acc: 0.7820\nepoch: 58\t, loss: 0.0032, test_acc: 0.7820\nepoch: 59\t, loss: 0.0032, test_acc: 0.7810\nepoch: 60\t, loss: 0.0031, test_acc: 0.7820\nepoch: 61\t, loss: 0.0030, test_acc: 0.7820\nepoch: 62\t, loss: 0.0029, test_acc: 0.7820\nepoch: 63\t, loss: 0.0028, test_acc: 0.7820\nepoch: 64\t, loss: 0.0028, test_acc: 0.7820\nepoch: 65\t, loss: 0.0027, test_acc: 0.7820\nepoch: 66\t, loss: 0.0026, test_acc: 0.7820\nepoch: 67\t, loss: 0.0026, test_acc: 0.7820\nepoch: 68\t, loss: 0.0025, test_acc: 0.7840\nepoch: 69\t, loss: 0.0025, test_acc: 0.7850\nepoch: 70\t, loss: 0.0024, test_acc: 0.7850\nepoch: 71\t, loss: 0.0024, test_acc: 0.7850\nepoch: 72\t, loss: 0.0024, test_acc: 0.7850\nepoch: 73\t, loss: 0.0023, test_acc: 0.7850\nepoch: 74\t, loss: 0.0023, test_acc: 0.7850\nepoch: 75\t, loss: 0.0022, test_acc: 0.7860\nepoch: 76\t, loss: 0.0022, test_acc: 0.7850\nepoch: 77\t, loss: 0.0022, test_acc: 0.7850\nepoch: 78\t, loss: 0.0021, test_acc: 0.7850\nepoch: 79\t, loss: 0.0021, test_acc: 0.7850\nepoch: 80\t, loss: 0.0021, test_acc: 0.7860\nepoch: 81\t, loss: 0.0020, test_acc: 0.7860\nepoch: 82\t, loss: 0.0020, test_acc: 0.7860\nepoch: 83\t, loss: 0.0020, test_acc: 0.7860\nepoch: 84\t, loss: 0.0020, test_acc: 0.7860\nepoch: 85\t, loss: 0.0019, test_acc: 0.7860\nepoch: 86\t, loss: 0.0019, test_acc: 0.7860\nepoch: 87\t, loss: 0.0019, test_acc: 0.7860\nepoch: 88\t, loss: 0.0019, test_acc: 0.7870\nepoch: 89\t, loss: 0.0018, test_acc: 0.7870\nepoch: 90\t, loss: 0.0018, test_acc: 0.7880\nepoch: 91\t, loss: 0.0018, test_acc: 0.7880\nepoch: 92\t, loss: 0.0018, test_acc: 0.7860\nepoch: 93\t, loss: 0.0017, test_acc: 0.7860\nepoch: 94\t, loss: 0.0017, test_acc: 0.7860\nepoch: 95\t, loss: 0.0017, test_acc: 0.7850\nepoch: 96\t, loss: 0.0017, test_acc: 0.7850\nepoch: 97\t, loss: 0.0017, test_acc: 0.7850\nepoch: 98\t, loss: 0.0017, test_acc: 0.7850\nepoch: 99\t, loss: 0.0016, test_acc: 0.7850\n"
    }
   ],
   "source": [
    "# this is a example of using gnn.GCNConv\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "import torch_geometric.data as gdata\n",
    "import torch_geometric.datasets as gdatasets\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "cora_path = \"pyg_datasets/cora\"\n",
    "cora = gdatasets.Planetoid(cora_path, \"cora\")\n",
    "cora = cora[0]\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_feature, hid_feature, out_feature):\n",
    "        nn.Module.__init__(self)\n",
    "        self.gcn_1 = gnn.GCNConv(in_feature, hid_feature)\n",
    "        self.gcn_2 = gnn.GCNConv(hid_feature, out_feature)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.gcn_1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.gcn_2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "net, cora = Net(1433, 16, 7).to(device), cora.to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    logit = net(cora.x, cora.edge_index)[cora.train_mask]\n",
    "    loss = F.nll_loss(logit, cora.y[cora.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    pred = net(cora.x, cora.edge_index)[cora.test_mask]\n",
    "    pred = pred.max(1)[1]\n",
    "    acc = pred.eq(cora.y[cora.test_mask]).sum().item() / cora.test_mask.sum().item()\n",
    "    print (\"epoch: {}\\t, loss: {:.4f}, test_acc: {:.4f}\".format(i, loss, acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch: 0\t, loss: 1.9335, test_acc: 0.1000\nepoch: 1\t, loss: 1.8521, test_acc: 0.1667\nepoch: 2\t, loss: 1.8398, test_acc: 0.0833\nepoch: 3\t, loss: 1.8276, test_acc: 0.1500\nepoch: 4\t, loss: 1.8382, test_acc: 0.1833\nepoch: 5\t, loss: 1.8182, test_acc: 0.2000\nepoch: 6\t, loss: 1.7966, test_acc: 0.1667\nepoch: 7\t, loss: 1.8075, test_acc: 0.2167\nepoch: 8\t, loss: 1.7327, test_acc: 0.2000\nepoch: 9\t, loss: 1.7697, test_acc: 0.2333\nepoch: 10\t, loss: 1.7340, test_acc: 0.3667\nepoch: 11\t, loss: 1.7433, test_acc: 0.2833\nepoch: 12\t, loss: 1.7725, test_acc: 0.2333\nepoch: 13\t, loss: 1.6981, test_acc: 0.2333\nepoch: 14\t, loss: 1.7049, test_acc: 0.3667\nepoch: 15\t, loss: 1.7954, test_acc: 0.1833\nepoch: 16\t, loss: 1.5727, test_acc: 0.2333\nepoch: 17\t, loss: 1.6031, test_acc: 0.3333\nepoch: 18\t, loss: 1.7465, test_acc: 0.3500\nepoch: 19\t, loss: 1.5707, test_acc: 0.3000\nepoch: 20\t, loss: 1.7060, test_acc: 0.3167\nepoch: 21\t, loss: 1.7750, test_acc: 0.2500\nepoch: 22\t, loss: 1.5671, test_acc: 0.3000\nepoch: 23\t, loss: 1.5639, test_acc: 0.3333\nepoch: 24\t, loss: 1.6557, test_acc: 0.3667\nepoch: 25\t, loss: 1.6018, test_acc: 0.2500\nepoch: 26\t, loss: 1.6749, test_acc: 0.2500\nepoch: 27\t, loss: 1.6249, test_acc: 0.2667\nepoch: 28\t, loss: 1.6330, test_acc: 0.3167\nepoch: 29\t, loss: 1.6527, test_acc: 0.3167\nepoch: 30\t, loss: 1.5577, test_acc: 0.3500\nepoch: 31\t, loss: 1.6763, test_acc: 0.3667\nepoch: 32\t, loss: 1.6061, test_acc: 0.3667\nepoch: 33\t, loss: 1.5117, test_acc: 0.3833\nepoch: 34\t, loss: 1.7553, test_acc: 0.3833\nepoch: 35\t, loss: 1.5468, test_acc: 0.3167\nepoch: 36\t, loss: 1.6121, test_acc: 0.3500\nepoch: 37\t, loss: 1.5593, test_acc: 0.3667\nepoch: 38\t, loss: 1.5578, test_acc: 0.3167\nepoch: 39\t, loss: 1.4911, test_acc: 0.3333\nepoch: 40\t, loss: 1.6122, test_acc: 0.3833\nepoch: 41\t, loss: 1.5523, test_acc: 0.3500\nepoch: 42\t, loss: 1.4960, test_acc: 0.2833\nepoch: 43\t, loss: 1.5295, test_acc: 0.3667\nepoch: 44\t, loss: 1.6140, test_acc: 0.3667\nepoch: 45\t, loss: 1.5666, test_acc: 0.3833\nepoch: 46\t, loss: 1.5833, test_acc: 0.4333\nepoch: 47\t, loss: 1.4843, test_acc: 0.3500\nepoch: 48\t, loss: 1.5178, test_acc: 0.4833\nepoch: 49\t, loss: 1.4627, test_acc: 0.4000\nepoch: 50\t, loss: 1.7181, test_acc: 0.3833\nepoch: 51\t, loss: 1.5530, test_acc: 0.3833\nepoch: 52\t, loss: 1.5469, test_acc: 0.4167\nepoch: 53\t, loss: 1.4480, test_acc: 0.3833\nepoch: 54\t, loss: 1.4740, test_acc: 0.4000\nepoch: 55\t, loss: 1.5525, test_acc: 0.3833\nepoch: 56\t, loss: 1.6229, test_acc: 0.4000\nepoch: 57\t, loss: 1.5572, test_acc: 0.4000\nepoch: 58\t, loss: 1.4583, test_acc: 0.4333\nepoch: 59\t, loss: 1.4862, test_acc: 0.4333\nepoch: 60\t, loss: 1.5586, test_acc: 0.4833\nepoch: 61\t, loss: 1.4724, test_acc: 0.3833\nepoch: 62\t, loss: 1.4478, test_acc: 0.4333\nepoch: 63\t, loss: 1.3095, test_acc: 0.4000\nepoch: 64\t, loss: 1.4990, test_acc: 0.4000\nepoch: 65\t, loss: 1.3114, test_acc: 0.4000\nepoch: 66\t, loss: 1.3036, test_acc: 0.4333\nepoch: 67\t, loss: 1.4126, test_acc: 0.4833\nepoch: 68\t, loss: 1.3047, test_acc: 0.3667\nepoch: 69\t, loss: 1.3002, test_acc: 0.4500\nepoch: 70\t, loss: 1.4454, test_acc: 0.3833\nepoch: 71\t, loss: 1.3319, test_acc: 0.3667\nepoch: 72\t, loss: 1.2904, test_acc: 0.4167\nepoch: 73\t, loss: 1.1735, test_acc: 0.3667\nepoch: 74\t, loss: 1.4170, test_acc: 0.3833\nepoch: 75\t, loss: 1.1892, test_acc: 0.3500\nepoch: 76\t, loss: 1.4599, test_acc: 0.3833\nepoch: 77\t, loss: 1.2196, test_acc: 0.3833\nepoch: 78\t, loss: 1.3578, test_acc: 0.4167\nepoch: 79\t, loss: 1.3342, test_acc: 0.3833\nepoch: 80\t, loss: 1.3283, test_acc: 0.4167\nepoch: 81\t, loss: 1.3372, test_acc: 0.4333\nepoch: 82\t, loss: 1.1629, test_acc: 0.3333\nepoch: 83\t, loss: 1.2680, test_acc: 0.4000\nepoch: 84\t, loss: 1.2172, test_acc: 0.3833\nepoch: 85\t, loss: 1.2192, test_acc: 0.3500\nepoch: 86\t, loss: 1.3005, test_acc: 0.4000\nepoch: 87\t, loss: 1.3010, test_acc: 0.4500\nepoch: 88\t, loss: 1.2899, test_acc: 0.3833\nepoch: 89\t, loss: 1.4239, test_acc: 0.4333\nepoch: 90\t, loss: 1.2186, test_acc: 0.3667\nepoch: 91\t, loss: 1.3733, test_acc: 0.3833\nepoch: 92\t, loss: 1.2635, test_acc: 0.3333\nepoch: 93\t, loss: 1.3786, test_acc: 0.3833\nepoch: 94\t, loss: 1.3367, test_acc: 0.4167\nepoch: 95\t, loss: 1.0872, test_acc: 0.4500\nepoch: 96\t, loss: 1.1411, test_acc: 0.4000\nepoch: 97\t, loss: 1.1328, test_acc: 0.4500\nepoch: 98\t, loss: 1.2630, test_acc: 0.4333\nepoch: 99\t, loss: 1.0744, test_acc: 0.3833\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_geometric\n",
    "import torch_geometric.nn as gnn\n",
    "import torch_geometric.data as gdata\n",
    "import torch_geometric.datasets as gdatasets\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "enzymes_path= \"pyg_datasets/enzymes\"\n",
    "enzymes = gdatasets.TUDataset(enzymes_path, \"ENZYMES\")\n",
    "enzymes = enzymes.shuffle()  # shuffle the dtataset\n",
    "enzymes_train = enzymes[:540]\n",
    "enzymes_test = enzymes[540:]\n",
    "enzymes_trainloader = gdata.DataLoader(enzymes_train, batch_size=60, shuffle=True)\n",
    "enzymes_testloader = gdata.DataLoader(enzymes_test, batch_size=60, shuffle=False)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        nn.Module.__init__(self)\n",
    "        self.conv1 = gnn.GraphConv(3, 128)\n",
    "        self.pool1 = gnn.TopKPooling(128, ratio=0.8)\n",
    "        self.conv2 = gnn.GraphConv(128, 128)\n",
    "        self.pool2 = gnn.TopKPooling(128, ratio=0.8)\n",
    "        self.conv3 = gnn.GraphConv(128, 128)\n",
    "        self.pool3 = gnn.TopKPooling(128, ratio=0.8)\n",
    "\n",
    "        self.lin1 = nn.Linear(256, 128)\n",
    "        self.lin2 = nn.Linear(128, 64)\n",
    "        self.lin3 = nn.Linear(64, 7)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, None, batch)\n",
    "        x1 = torch.cat([gnn.global_max_pool(x, batch), gnn.global_mean_pool(x, batch)], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x2 = torch.cat([gnn.global_max_pool(x, batch), gnn.global_mean_pool(x, batch)], dim=1)\n",
    "        \n",
    "        x = F.relu(self.conv3(x, edge_index))\n",
    "        x, edge_index, _, batch, _, _ = self.pool2(x, edge_index, None, batch)\n",
    "        x3 = torch.cat([gnn.global_max_pool(x, batch), gnn.global_mean_pool(x, batch)], dim=1)\n",
    "        \n",
    "        x = x1 + x2 + x3\n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.lin2(x))\n",
    "        x = F.log_softmax(self.lin3(x), dim=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=5e-4)\n",
    "\n",
    "for i in range(100):  # one epoch\n",
    "    for one_train_batch in enzymes_trainloader:\n",
    "        net.train()\n",
    "        optimizer.zero_grad()\n",
    "        one_train_batch = one_train_batch.to(device)\n",
    "        logit = net(one_train_batch.x, one_train_batch.edge_index, one_train_batch.batch)\n",
    "        loss = F.nll_loss(logit, one_train_batch.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    for one_test_batch in enzymes_testloader:\n",
    "        net.eval()\n",
    "        one_test_batch = one_test_batch.to(device)\n",
    "        pred = net(one_test_batch.x, one_test_batch.edge_index, one_test_batch.batch)\n",
    "        pred = pred.max(1)[1]\n",
    "        acc = pred.eq(one_test_batch.y).sum().item() / len(one_test_batch.y)\n",
    "    print (\"epoch: {}\\t, loss: {:.4f}, test_acc: {:.4f}\".format(i, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}